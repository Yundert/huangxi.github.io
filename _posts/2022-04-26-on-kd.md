---
id: 20220526
title: 如何“授之以渔”？——浅谈知识蒸馏
pdate: "26 April 2022"
author: Xi
comments: true
layout: post
categories: AI
guid: https://yundert.github.io/2022/04/26/on-kd/
permalink: /2022/04/26/on-kd/
abstract: 一篇梳理知识蒸馏技术的短文。
---

| ![front image](/images/20220426fig1.png) |
|:--:|
| <b style="font-size:12px"> An example of teacher-student interaction for knowledge distillation (source from the Internet) </b>|


### 背景与动因

过去十余年，机器学习技术（尤其是深度学习）在数据、算法、算力三要素的协同促进下，掀起了新一波的人工智能热潮。近来，人工智能也从云端等坐拥充足资源的集中式计算环境，逐步下沉至异构边缘设备共生的网络边缘生态圈，与边缘计算、物联网、无人驾驶等技术融合，赋能不同垂直领域的业务场景，实现更精细的服务定制、质量管理、决策辅助及流程的降本增效。


这其中常见的核心问题之一，便是”如何实现已训练模型在生产环境下的部署”。其挑战源于训练与生产环境的差异。差异因素众多，机器学习要素之二——“领域”（domain）和“模型”（model）——便包含其中。


为了便于理解，我们以监督学习为例。监督学习中，“领域”刻画了学习任务的上下文，其包括任务所感兴趣的所有事件观察的集合，和定义其上的有效分布；而“模型”则刻画了这些事件观察中样本与标签之间的潜在关联。而学习的目标便是利用产生于给定“领域”的有限样本，还原出背后关联。


| ![front image](/images/20220426fig2.png) |
|:--:|
| <b style="font-size:12px"> (Source from the Internet) </b>|


相应地，模型迁移与部署过程中，同一监督学习任务在领域上的差异可产生于生产环境的复杂性（如实际场景更复杂，许多事件观察鲜见于训练环节）或事件分布的差异；对于模型差异，主要源头为部署时的资源限制，譬如实际设备难以满足模型推理的资源需求，使部署模型不得不有别于原模型。这两类差异的组合，衍化出了不同的技术范畴：1）当领域不同而模型相同时，关注点在如何完成从原领域到部署时目标领域的适配（domain adaption）；2）当领域不变而模型有差异，重点在“如何完成从训练模型到部署模型的转化”；3）更一般的情况（甚至任务有别时），则属于迁移学习（transfer learning）的范畴。本文关注第二类情况。


实现“训练模型到部署模型的转化”有两大常见手段，一是直接对原（训练）模型进行压缩，如通过剪枝（pruning）与量化（quantization）等技术。二是将原模型的“知识”进行“提炼”并“传授”给预部署的目标模型（另一类潜在场景即维护原模型与模型部署主体不同，而后者数据匮乏，无法进行有效训练）。本质上，第二种手段是通过建立在原模型与目标模型间的间接知识传递实现的，是谓“授之以渔”。实现该手段的技术方法的集合通常被称为知识蒸馏（knowledge distillation）。


关于知识蒸馏，几个核心问题包括：

- 知识蒸馏的基本框架是怎样的？
- 所传递的知识主要有哪些形式？
- 知识蒸馏的主要技术思想？

围绕上述问题，我们接下来将对知识蒸馏的基本元素展开介绍。
<br><br>

### 要素其一：基本架构

常见的知识蒸馏框架如下图所示。

| ![front image](/images/20220426fig3.png) |
|:--:|
| <b style="font-size:12px"> (Source from the Internet) </b>|

知识蒸馏包括“教师模型”（teacher model）和“学生模型”（student model）。前者是基于已有数据集完成训练的模型，后者是未训练或未完成训练的模型。在知识蒸馏过程中，教师模型首先采样小部分数据，结合数据通过某种蒸馏机制得到模型知识，接着将知识与采样数据一同传给学生模型。学生模型通过优化自身参数，在给定相同数据前提下，产生尽可能与该知识相匹配的输出。这边优化的性能指标一般有两类：一是尽可能与教师模型输出匹配，此类指标称为保真度（fidelity）；二是学生模型的泛化性能。两者都可以是直接优化目标的一部分，但需要注意的是，后者才是最终目标。

以上过程中，知识可以单向传递，也可引入闭环控制（学生通过提供反馈信息，让教师进一步优化蒸馏策略）。更一般地，蒸馏过程可有多个教师或多个学生同时参与，或二者兼有。这里不作讨论。
<br><br>

### 要素其二：知识形式

知识的形式是知识蒸馏的核心要素之一。其不仅决定了传递的信息量（进而影响学生的性能），同时也影响蒸馏方法的设计。暂抛开知识蒸馏，机器学习中两类常见的知识载体，一是数据，二是模型。前者可以由现实观测所得，或由模型合成。后者以数据为原料，被塑造为解决特定任务的工具。通过传递数据进行知识迁移不难理解。至于模型，例如，在联邦学习中，客户端周期性地将本地训练模型上传到服务器，并由后者完成模型聚合，得到全局模型。此过程便可视为知识传递的一类方式。然而，此例仅限于模型（参数）本身。更广义上讲，模型知识应定义为模型产物。其除了模型本身（所有参数），还可包含模型训练（如梯度更新）以及模型推理过程的产物。

![front image](/images/20220426fig4.png) 

知识蒸馏中，知识主要以模型推理的中间产物为载体。常见有三大类，分别是基于响应的知识（response-based knowledge）、基于特征的知识（feature-based knowledge）和基于关联的知识（relation-based knowledge）。下面我们对这三大类知识进行简要介绍。
<br><br>

- 基于响应的知识

基于响应的知识又被称为基于预测（prediction-based）或基于激活（activation-based）的知识。在监督学习中，其被定义为（前向）神经网络最终全连接层的输出。

![front image](/images/20220426fig5.png) 

比如，在面向多分类问题的神经网络中，响应定义为给定输入时，其推理产生的关于各类别评分的原始输出矢量。这些输出通常会经过正规化运算（如softmax）转化为有效的随机分布，以刻画该输入属于各类别的似然率。

值得指出的是，正规化运算可能造成有价值信息的损失。考虑上图中的例子。该例中，相较正规化处理前，关于第2类到第k类的评分差异性在正规化处理后显著降低。尽管这些差异的变化对此次推理的最后结果没有影响，然而它们却蕴含了该模型推理过程关于各类判断的重要信息，如哪些类相比其他类与该输入有着更大的相似性。这类信息通常被称为“暗知识”（dark knowledge），其编码了与模型泛化性相关的信息。这类信息作为模型推理的中间产物之一，被运用在基于响应的知识蒸馏（response-based knowledge distillation）中。

直观来说，教师在向学生传授知识时，提供数据样本后（类比习题集），相比仅告诉学生最终答案，进一步提供其对每个类（选项）的判断情况更有望给学生的泛化学习能力带来提升。
<br><br>

- 基于特征的知识

尽管响应提供了有关推理过程的知识，其局限性却不容忽视。一方面，其局限于前向神经网络模型，特别是分类问题，难以适配到其它类型的神经网络。另一方面，利用最终层的输出作为推理过程的产物加以利用，那么意味着神经网络某些中间层的输出也编码着重要的推理过程信息。

![front image](/images/20220426fig6.png) 

基于以上思想，特征被定义为神经网络模型中某些层的输出或者输出的映射。例如，在视觉任务中，某些层的输出（特征图）表征了不同抽象程度下的特征提取情况。这些信息不仅有望为学生模型中间层的参数优化提供指导，同时也拓宽了知识蒸馏的适用场景（不再只限分类问题）。


直观来说，教师在向学生传授知识时，相比只提供关于最终输出的推理信息，提供中间关键步骤的推理信息有望进一步提升学生的学习表现。

考虑到教师模型往往比学生模型更强大（有着更宽或更深的网络），知识蒸馏的方法设计也因此引入了更丰富的自由度。例如：
特征输出层的选择：教师模型哪些层的输出能够为学生模型提供最用的知识？

特征输出层的匹配：这些层如何匹配到学生模型的层，进而让后者有针对性地进行模型参数的优化？

围绕这些自由度的方法设计将在之后提到。
<br><br>

- 基于关联的知识

相较基于特征的知识而言，我们可以进一步考虑教师模型在推理过程中的高阶知识。大体分为两类：一类为给定同一数据样本，模型推理过程中不同层输出之间的关联信息；另一类为同一层在模型推理时对应不同数据样本的输出之前的关联信息。这些知识通常被称为基于关联的知识。

![front image](/images/20220426fig7.png) 

直观来说，教师在向学生传授知识时，对于同一个数据样本（类比于习题），提供某些关键推理步骤之间的关联，或是就同一推理步骤，对给定不同数据样本时输出的差异提供解释，那么对学生的学习表现将有所裨益。

与基于特征的知识类似，“关联”如何刻画是相关知识蒸馏方法设计的重点。感兴趣可以参考文末论文链接作进一步探究。
<br><br>

### 要素其三：蒸馏方法及其主要思想

知识蒸馏方法可以从不同视角进行分类。本文介绍常见的四类视角。

- 按知识形式分类

上节中，我们介绍了知识蒸馏中常见知识形式。相应地，知识蒸馏技术的设计也因知识形式而异。

在基于响应的知识蒸馏中，教师模型采样一批数据样本，产生对应的响应输出。接着，教师模型将数据样本与响应输出传递给学生模型，学生模型通过优化自身参数产生尽可能匹配的响应输出（此处可以引入正则化提升泛化性能）。该过程中，学生不需提供反馈。

在基于特征的知识蒸馏中，蒸馏方法设计需要回答两个问题——“教师模型哪些层提供了重要信息”以及“如何将这些层与学生模型中的层进行匹配”。对此，一般有两类解决方案。一类是采用启发式方法手动标记输出层以及与学生模型的匹配关系。许多现有工作都属于此类。另一类是通过引入额外的选择模块（由神经网络实现，如Chen et al. 在AAAI 2021的工作），一方面基于教师与学生模型的各层输出，为后者决定特征输出层的选择与匹配，另一方面采集学生模型的反馈信息，对该模块进行动态更新。对于此类方法，目前仍假设学生模型中的所有层在教师模型中都有对应的匹配，然而这未必是最有效的匹配方式。一种观点是：学生模型只有部分层需要匹配，其余层保留一定的自由度，反而有利于最终泛化性能的提升。

在基于关联的知识蒸馏中，蒸馏方法的关注点集中在如何设计有效的特征提取器，有效刻画输入同一样本时不同层输出之间（或输入不同样本时同一层输出之间）的关联。此类方法设计尽管较前一类更为复杂，但关注问题类似，即哪些关联编码了更丰富的推理信息，以及如何将这些关联匹配到学生模型不同层的关联，进而为后者提供参数优化的方向。
<br><br>

- 按学习过程分类

除了知识形式，知识蒸馏技术也可依据蒸馏过程中教师模型的变化进行分类。

离线知识蒸馏（offline knowledge distillation）：在经典的知识蒸馏技术中，教师通常被假定为完成良好训练（well-trained）的模型。此类技术的重点在于如何在教师模型给定情况下，提取并传递知识用于指导学生模型的参数优化。这类不要求教师模型改变的技术，被称为离线知识蒸馏技术。

在线知识蒸馏（online knowledge distillation）：某些情况下，教师模型在知识蒸馏过程中也需要对自身参数进行更新，此类技术被称为在线知识蒸馏。比如，当存在多个智能体与环境互动，每个智能体基于自身局部感知数据训练出本地模型。此时，每个智能体可以借助与其它智能体间关于环境的知识共享，更好地完成自身任务。此例中，教师与学生的角色是相对的，教师在蒸馏知识的同时，也需要对自身模型参数进行调整。感兴趣可参考文末链接。

自蒸馏技术（self-distillation）：除了离线与在线形式，事实上还有一种中间形式。即教师模型存在多个版本，某些版本变化（参数更新），而其它版本不变。此类技术一般称为自蒸馏技术。自蒸馏技术可以用于不同的目的。
模型压缩：为了实现模型压缩，可以将训练好的某一模型作为教师模型，同时将其简化后的版本（如通过剪枝，一般用于部署）作为学生模型。为了保证后者能够达到接近于前者的性能，可考虑采用蒸馏技术达到此目的。

优化训练：此类技术用于某一模型的训练过程中。训练时，该模型每迭代一定次数后，便将其快照（snapshot）固定为一个教师模型。在往后的训练中，模型更新过程将结合与各教师模型间的知识蒸馏，减少训练的方差并避免过拟合。
<br><br>

- 按原数据的可访问性分类

知识蒸馏也可按训练数据的可访问性进行分类。

数据驱动的知识蒸馏（data-driven knowledge distillation）：对于常见的知识蒸馏，原始数据集在蒸馏过程中是可以使用的（可访问的）。教师模型通过采样数据样本，产生知识指导学生模型的训练。此类技术被称为数据驱动的知识蒸馏。

无数据知识蒸馏（data-free knowledge distillation）：某些情况下，原始数据集可能无法访问（如涉及敏感信息、隐私等）。此时需利用教师模型，逆向合成数据集，其仅与原数据集在分布上近似。接着，教师模型借助合成数据集（类似前一类方法），向学生模型输出蒸馏得到的知识。此类技术被称为无数据知识蒸馏。
<br><br>

- 按学习范式分类

现有知识蒸馏技术主要面向有监督学习，尤其是其中的分类问题。然而，该技术的发展并不局限于此。近年来，研究人员也在积极探索针对有监督学习中回归问题，以及半监督、无监督学习的知识蒸馏技术。相关的技术方法可参考文末引用。
<br><br>

### 展望

对于知识蒸馏，本文有两点评述。

其一，知识蒸馏领域正快速发展，相关工作已数万篇有余。但对于知识蒸馏技术本身何种条件下有效，为何有效，以及决定其有效性的关键性因素等问题，尽管部分研究者已开展初步探索（参考文末四篇引用），更全面的系统性理论解释仍有待发掘。

其二，知识蒸馏的早期研究（如Hinton等人）是以深度神经网络的模型压缩作为动因而提出的。这主导了后续研究的路径依赖——当前知识蒸馏技术与神经网络模型的结构特性是紧密耦合的。然而，神经网络并非编码知识的唯一选项。由此，一个基础性问题便是：除了基于神经网络的知识蒸馏，面对非神经网络模型，如何实现知识蒸馏？这背后更根本的问题是：模型间的知识蒸馏如何理解？这些问题仍有待解答。
<br><br>

### 相关引用

Liu, Y., Zhang, W., Wang, J., & Wang, J. (2021). Data-Free Knowledge Transfer: A Survey. arXiv preprint arXiv:2112.15278.

Gou, J., Yu, B., Maybank, S. J., & Tao, D. (2021). Knowledge Distillation: A Survey. International Journal of Computer Vision, 129(6), 1789-1819.

Alkhulaifi, A., Alsahli, F., & Ahmad, I. (2021). Knowledge Distillation in Deep Learning and Its Applications. PeerJ Computer Science, 7.

Wang, L., & Yoon, K. J. (2021). Knowledge Distillation and Student-teacher Learning for Visual Intelligence: A Review and New Outlooks. IEEE Transactions on Pattern Analysis and Machine Intelligence (Early Access).

Chen, D., Mei, J. P., Zhang, Y., Wang, C., Wang, Z., Feng, Y., & Chen, C. (2021). Cross-layer Distillation with Semantic Calibration. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35, No. 8, pp. 7028-7036.

Stanton, S., Izmailov, P., Kirichenko, P., Alemi, A. A., & Wilson, A. G. (2021). Does knowledge distillation really work?. Advances in Neural Information Processing Systems, 34.

Allen-Zhu, Z., & Li, Y. (2020). Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. arXiv preprint arXiv:2012.09816.

Tang, J., Shivanna, R., Zhao, Z., Lin, D., Singh, A., Chi, E. H., & Jain, S. (2020). Understanding and improving knowledge distillation. arXiv preprint arXiv:2002.03532.

Phuong, M., & Lampert, C. (2019, May). Towards understanding knowledge distillation. In International Conference on Machine Learning (pp. 5142-5151). PMLR.


<div id="disqus_thread"></div>
<script>
    (function(){
        var elems = document.getElementsByClassName("view");
        elems[elems.length-1].remove();
    })();
</script>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://yundert-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<script id="dsq-count-scr" src="//yundert-github-io.disqus.com/count.js" async></script>